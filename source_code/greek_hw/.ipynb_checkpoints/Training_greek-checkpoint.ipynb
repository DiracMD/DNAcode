{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "#from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D\n",
    "from keras.layers import AveragePooling2D,MaxPooling2D,Dropout,GlobalMaxPool2D,GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "#inputshape=(height,width,deep)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import h5py\n",
    "import os \n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import numpy as np\n",
    "import PIL.ImageOps\n",
    "import random\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "def get_layer_output(model,x,index=-1):\n",
    "    layer = K.function([model.input],[model.layers[index].output])\n",
    "    return layer([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File(\"greek_dataset_20200722-5.h5\",'r')\n",
    "X_train = np.array(train_dataset['x_train'][:])\n",
    "Y_train = np.array(train_dataset['y_train'][:])\n",
    "X_test = np.array(train_dataset['x_test'][:])\n",
    "Y_test = np.array(train_dataset['y_test'][:])\n",
    "train_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "class_names=[\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"theta\",\"eta\",]\n",
    "print(len(X_train))\n",
    "for i in range(64):\n",
    "    print(np.squeeze(X_train[i]),np.sum(X_train[i]==1))\n",
    "    print(class_names[int(Y_train[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"number of traning examples = \"+str(X_train.shape[0]))\n",
    "print(\"number of test examples = \"+str(X_test.shape[0]))\n",
    "print(\"X_train shape:\" + str(X_train.shape))\n",
    "print(\"Y_train shape:\" + str(Y_train.shape))\n",
    "print(\"X_test shape:\" + str(X_test.shape))\n",
    "print(\"Y_test shape:\" + str(Y_test.shape))\n",
    "train_data=(X_train,Y_train)\n",
    "test_data=(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model\n",
    "def Pimodel(input_shape):\n",
    "    X_input = Input(shape=input_shape)\n",
    "    X=ZeroPadding2D(padding=(1,1))(X_input)\n",
    "    X=Conv2D(1,kernel_size=(3,6),strides=(3,6),use_bias=None)(X)\n",
    "    X=Activation('relu')(X)\n",
    "    #X=Conv2D(1,kernel_size=(2,3),strides=(2,2))(X)\n",
    "    #X=Activation(\"sigmoid\")(X)\n",
    "    X=Flatten()(X)\n",
    "    Y=Activation('softmax')(X)\n",
    "    model=Model(inputs=X_input,outputs=Y,name=\"JSmodel\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picmodel = Pimodel((10,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "picmodel.compile(optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08,decay=0.0),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "picmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history=picmodel.fit(x=X_train,y=Y_train,batch_size=150,epochs=750,validation_data=(X_test,Y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = picmodel.evaluate(x=X_test,y=Y_test)\n",
    "print('Test Loss=' + str(preds[0]))\n",
    "print(\"Test Accuracy =\"+str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.text(100,0.4,\"accuracy=\"+str(preds[1]))\n",
    "plt.ylim((0, 1)) # Uncomment this when showing you model for pay raise\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig(\"accuary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.text(100,1,\"loss=\"+str(preds[0]))\n",
    "plt.legend(['train', 'test'], loc=\"best\")\n",
    "plt.savefig(\"loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr=np.load(\"./dataset2/alpha/alpha_original_image_231.jpg_1118e2d6-cbfd-4a73-9dec-8aa773911a93.jpg.npy\")\n",
    "x=np.expand_dims(arr,axis=0)\n",
    "x=np.expand_dims(x,axis=3)\n",
    "print(picmodel.predict(x))\n",
    "class_names[np.argmax(picmodel.predict(x))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "picmodel.save(\"greek_model_20200729.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picmodel=load_model(\"./greek_model_20200729.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "def get_layer_output(model,x,index=-1):\n",
    "    layer = K.function([model.input],[model.layers[index].output])\n",
    "    return layer([x])[0]\n",
    "input_y=np.load(\"./dataset2/alpha/alpha_original_image_231.jpg_7a29d59a-6d8d-47bf-a566-01f51e71f29e.jpg.npy\")\n",
    "input_x = np.expand_dims(input_y,axis=0)\n",
    "input_x = np.expand_dims(input_x,axis=3)\n",
    "layer_0 = get_layer_output(picmodel,input_x,index=0)\n",
    "layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "print(np.squeeze(layer_0))\n",
    "print(np.squeeze(layer_1))\n",
    "print(np.squeeze(layer_2))\n",
    "print(np.squeeze(layer_3))\n",
    "print(np.squeeze(layer_4))\n",
    "print(np.squeeze(layer_5))\n",
    "class_names[np.argmax(picmodel.predict(input_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_weights=np.squeeze(picmodel.get_weights())\n",
    "#print(kernel_weights.flatten())\n",
    "a1=np.squeeze(kernel_weights)[0:3,0:6].flatten()\n",
    "print(kernel_weights)\n",
    "N=abs(a1.flatten())[np.argmax(abs(a1.flatten()))]\n",
    "print(N)\n",
    "a1_nor=a1.flatten()/N\n",
    "print(a1_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "picmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the layer output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import numpy\n",
    "import random\n",
    "import os,sys\n",
    "def get_layer_output(model,x,index=0):\n",
    "    layer = K.function([model.input],[model.layers[index].output])\n",
    "    return layer([x])[0]\n",
    "path=\"./dataset2/alpha/\"\n",
    "imlist=os.listdir(path)\n",
    "#rlist=random.sample(imlist,1000)\n",
    "one_t=[]\n",
    "One_t=[]\n",
    "fn_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/alpha/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    one_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"alpha\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        fn_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        One_t.append(elef) \n",
    "one1=collections.Counter(one_t)\n",
    "print(len(one_t),collections.Counter(one_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#path=\"./dataset/A/\"\n",
    "#imlist=os.listdir(path)\n",
    "for elef in random.sample(One_t,301):\n",
    "    input_x=np.load(r\"./dataset1/alpha/\"+elef)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,2)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P2-\"+elef,inputy1)\n",
    "    print(input_x,np.sum(input_x==1))\n",
    "    print(inputy1,np.sum(inputy1==1))\n",
    "    #print(arr)\n",
    "    #np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][0]\n",
    "fn_t.sort(key=takesecond,reverse=True)\n",
    "for i in fn_t:\n",
    "    input_y=np.load(\"./dataset2/alpha/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "for elef in train:\n",
    "    im=Image.fromarray(np.load(\"./dataset/one/\"+elef))\n",
    "    im=PIL.ImageOps.invert(im)\n",
    "    im2=im.resize((10,10))\n",
    "    L=np.array(im2).flatten()\n",
    "    for i in range(100):\n",
    "        if L[i]>200:\n",
    "            L[i]=0\n",
    "        else:\n",
    "            L[i]=1\n",
    "    input_y=np.array(L).reshape(10,10)\n",
    "    print(input_y)\n",
    "    np.save(elef,input_y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/beta/\"\n",
    "imlist=os.listdir(path)\n",
    "#rlist=random.sample(imlist,1000)\n",
    "two_t=[]\n",
    "Two_t=[]\n",
    "beta_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/beta/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    two_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_4)]==\"beta\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        beta_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Two_t.append(elef)\n",
    "two1=collections.Counter(two_t)\n",
    "print(len(two_t),collections.Counter(two_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][1]\n",
    "beta_t.sort(key=takesecond,reverse=True)\n",
    "for i in beta_t:\n",
    "    input_y=np.load(\"./dataset2/beta/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Two_t,147):\n",
    "    #print(elef)\n",
    "    input_x=np.load(\"./dataset1/beta/\"+elef)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,2)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P1-\"+elef,inputy1)\n",
    "    print(input_x,np.sum(input_x==1))\n",
    "    print(inputy1,np.sum(inputy1==1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Two_t,238):\n",
    "    #print(elef)\n",
    "    input_x=np.load(\"./10dataset/two/\"+elef)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,1)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P1-\"+elef,inputy1)\n",
    "    print(input_x)\n",
    "    print(inputy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/gamma/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "three_t=[]\n",
    "Three_t=[]\n",
    "gamma_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/gamma/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    three_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_4)]==\"gamma\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        gamma_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Three_t.append(elef)\n",
    "three1=collections.Counter(three_t)\n",
    "print(len(three_t),collections.Counter(three_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][2]\n",
    "gamma_t.sort(key=takesecond,reverse=True)\n",
    "for i in gamma_t:\n",
    "    input_y=np.load(\"./dataset2/gamma/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Three_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset1/gamma/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/delta/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "four_t=[]\n",
    "Four_t=[]\n",
    "delta_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/delta/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    four_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_2)]==\"delta\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        delta_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_4)])\n",
    "        Four_t.append(elef)\n",
    "four1=collections.Counter(four_t)\n",
    "print(len(four_t),collections.Counter(four_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][3]\n",
    "delta_t.sort(key=takesecond,reverse=True)\n",
    "for i in delta_t:\n",
    "    input_y=np.load(\"./dataset2/delta/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Four_t,281):\n",
    "    #print(elef)\n",
    "    input_x=np.load(\"./dataset1/delta/\"+elef)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,2)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P2-\"+elef,inputy1)\n",
    "    print(input_x,np.sum(input_x==1))\n",
    "    print(inputy1,np.sum(inputy1==1))\n",
    "    #np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/epsilon/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "five_t=[]\n",
    "Five_t=[]\n",
    "epsilon_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/epsilon/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    five_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_4)]==\"epsilon\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        epsilon_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_4)])\n",
    "        Five_t.append(elef)\n",
    "five1=collections.Counter(five_t)\n",
    "print(len(five_t),collections.Counter(five_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][4]\n",
    "epsilon_t.sort(key=takesecond,reverse=True)\n",
    "for i in epsilon_t:\n",
    "    input_y=np.load(\"./dataset2/epsilon/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Five_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset1/epsilon/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/zeta/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "six_t=[]\n",
    "Six_t=[]\n",
    "zeta_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/zeta/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    six_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"zeta\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        zeta_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Six_t.append(elef)\n",
    "six1=collections.Counter(six_t)\n",
    "print(len(six_t),collections.Counter(six_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][5]\n",
    "zeta_t.sort(key=takesecond,reverse=True)\n",
    "for i in zeta_t:\n",
    "    input_y=np.load(\"./dataset2/zeta/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Six_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset1/zeta/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ele in random.sample(Six_t,486):\n",
    "    input_x=np.load(r\"./dataset2/F/\"+ele)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,1)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P\"+ele,input_x)\n",
    "    print(input_x)\n",
    "    print(inputy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/eta/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "seven_t=[]\n",
    "Seven_t=[]\n",
    "eta_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/eta/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    seven_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"eta\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        eta_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Seven_t.append(elef)\n",
    "seven1=collections.Counter(seven_t)\n",
    "print(len(seven_t),collections.Counter(seven_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][7]\n",
    "eta_t.sort(key=takesecond,reverse=True)\n",
    "for i in eta_t:\n",
    "    input_y=np.load(\"./dataset2/eta/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Seven_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset1/eta/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset2/theta/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "eight_t=[]\n",
    "Eight_t=[]\n",
    "theta_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset2/theta/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    eight_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"theta\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        theta_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Eight_t.append(elef)\n",
    "eight1=collections.Counter(eight_t)\n",
    "print(len(eight_t),collections.Counter(eight_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][6]\n",
    "theta_t.sort(key=takesecond,reverse=True)\n",
    "for i in theta_t:\n",
    "    input_y=np.load(\"./dataset2/theta/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\",i[1])\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Eight_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset1/theta/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Eight_t,42):\n",
    "    #print(elef)\n",
    "    input_x=np.load(\"./10dataset/eight/\"+elef)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,1)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P2\"+elef,inputy1)\n",
    "    print(input_x)\n",
    "    print(inputy1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path=\"./dataset/nine/\"\n",
    "imlist=os.listdir(path)\n",
    "rmlist=random.sample(imlist,1000)\n",
    "nine_t=[]\n",
    "Nine_t=[]\n",
    "for elef in imlist:\n",
    "    im=Image.fromarray(np.load(\"./dataset/nine/\"+elef))\n",
    "    imt=im.resize((10,10))\n",
    "    arr=np.array(imt)\n",
    "    input_y=np.where(arr<1,arr,1)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    nine_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"eight\":\n",
    "        print(elef,layer_5,class_names[np.argmax(layer_5)])\n",
    "        Nine_t.append(elef)\n",
    "nine1=collections.Counter(nine_t)\n",
    "print(len(nine_t),collections.Counter(nine_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ele in Life_T:\n",
    "    input_x = np.array(np.load(r\"./dataset_200426/life/\"+ele))\n",
    "    print(input_x,np.sum(input_x==1))\n",
    "    np.save(\"o\"+ele,input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#dict1 = pd.DataFrame(pd.Series(f1),columns=[\"numbers\"],index=[\"fire\",\"soil\",\"water\",\"wood\",\"gas\",\"sky\",\"human\",\"life\"])\n",
    "x=[\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"]\n",
    "dict1=pd.Series(one1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict1=dict1.fillna(0)\n",
    "#print(dict1)\n",
    "#print(dict1.fillna(0))\n",
    "#dict1[np.squeeze(np.argwhere(np.isnan(dict1)))]=0\n",
    "\n",
    "dict2=pd.Series(two1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "#dict2[np.squeeze(np.argwhere(np.isnan(dict2)))]=0\n",
    "dict2=dict2.fillna(0)\n",
    "dict3=pd.Series(three1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict3=dict3.fillna(0)\n",
    "#dict3[np.squeeze(np.argwhere(np.isnan(dict3)))]=0\n",
    "dict4=pd.Series(four1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict4=dict4.fillna(0)\n",
    "#dict4[np.squeeze(np.argwhere(np.isnan(dict4)))]=0\n",
    "dict5=pd.Series(five1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict5=dict5.fillna(0)\n",
    "#dict5[np.squeeze(np.argwhere(np.isnan(dict5)))]=0\n",
    "dict6=pd.Series(six1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict6=dict6.fillna(0)\n",
    "#dict6[np.squeeze(np.argwhere(np.isnan(dict6)))]=0\n",
    "dict7=pd.Series(seven1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict7=dict7.fillna(0)\n",
    "#dict7[np.squeeze(np.argwhere(np.isnan(dict7)))]=0\n",
    "dict8=pd.Series(eight1).reindex([\"alpha\",\"beta\",\"gamma\",\"delta\",\"epsilon\",\"zeta\",\"eta\",\"theta\"])\n",
    "dict8=dict8.fillna(0)\n",
    "#dict8[np.squeeze(np.argwhere(np.isnan(dict8)))]=0\n",
    "plt.figure(figsize=(24,10))\n",
    "\n",
    "plt.subplot(241)\n",
    "sns.barplot(x,y=dict1,palette=\"rocket\")\n",
    "for i,j in zip(range(8), dict1):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#plt.bar(x,dict1,color=\"red\")\n",
    "plt.title(\"alpha regonition\")\n",
    "\n",
    "plt.subplot(242)\n",
    "sns.barplot(x,dict2,palette=\"rocket\")\n",
    "plt.title(\"beta regonition\")\n",
    "for i,j in zip(range(8), dict2):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict2)\n",
    "\n",
    "plt.subplot(243)\n",
    "#plt.bar(x,dict3,color=\"brown\")\n",
    "sns.barplot(x,dict3,palette=\"rocket\")\n",
    "plt.title(\"gamma regonition\")\n",
    "for i,j in zip(range(8), dict3):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "\n",
    "plt.subplot(244)\n",
    "#plt.bar(x,dict4,color=\"blue\")\n",
    "sns.barplot(x,dict4,palette=\"rocket\")\n",
    "plt.title(\"delta regonition\")\n",
    "for i,j in zip(range(8), dict4):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(245)\n",
    "#plt.bar(x,dict5,color=\"green\")\n",
    "sns.barplot(x,dict5,palette=\"rocket\")\n",
    "plt.title(\"epsilon regonition\")\n",
    "for i,j in zip(range(8), dict5):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(246)\n",
    "#plt.bar(x,dict6,color=\"orange\")\n",
    "sns.barplot(x,dict6,palette=\"rocket\")\n",
    "plt.title(\"zeta regonition\")\n",
    "for i,j in zip(range(8), dict6):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(247)\n",
    "#plt.bar(x,dict7,color=\"black\")\n",
    "sns.barplot(x=x, y=dict7, palette=\"rocket\")\n",
    "plt.title(\"eta regonition\")\n",
    "for i,j in zip(range(8), dict7):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(248)\n",
    "#plt.bar(x,dict8,color=\"purple\")\n",
    "sns.barplot(x,dict8,palette=\"rocket\")\n",
    "plt.title(\"theta regonition\")\n",
    "for i,j in zip(range(8), dict8):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "#list1=dict1.values.tolist()\n",
    "plt.savefig(\"regonition.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
