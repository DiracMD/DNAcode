{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "#from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D\n",
    "from keras.layers import AveragePooling2D,MaxPooling2D,Dropout,GlobalMaxPool2D,GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "#inputshape=(height,width,deep)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import h5py\n",
    "import os \n",
    "from PIL import Image\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "import numpy as np\n",
    "import PIL.ImageOps\n",
    "import random\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "def get_layer_output(model,x,index=-1):\n",
    "    layer = K.function([model.input],[model.layers[index].output])\n",
    "    return layer([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File(\"alphabet_dataset_20200725-1.h5\",'r')\n",
    "X_train = np.array(train_dataset['x_train'][:])\n",
    "Y_train = np.array(train_dataset['y_train'][:])\n",
    "X_test = np.array(train_dataset['x_test'][:])\n",
    "Y_test = np.array(train_dataset['y_test'][:])\n",
    "train_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "class_names=[\"B\",\"A\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",]\n",
    "print(len(X_train))\n",
    "for i in range(64):\n",
    "    print(np.squeeze(X_train[i]),np.sum(X_train[i]==1))\n",
    "    print(class_names[int(Y_train[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"number of traning examples = \"+str(X_train.shape[0]))\n",
    "print(\"number of test examples = \"+str(X_test.shape[0]))\n",
    "print(\"X_train shape:\" + str(X_train.shape))\n",
    "print(\"Y_train shape:\" + str(Y_train.shape))\n",
    "print(\"X_test shape:\" + str(X_test.shape))\n",
    "print(\"Y_test shape:\" + str(Y_test.shape))\n",
    "train_data=(X_train,Y_train)\n",
    "test_data=(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a model\n",
    "def Pimodel(input_shape):\n",
    "    X_input = Input(shape=input_shape)\n",
    "    X=ZeroPadding2D(padding=(1,1))(X_input)\n",
    "    X=Conv2D(1,kernel_size=(3,6),strides=(3,6),use_bias=None)(X)\n",
    "    X=Activation('relu')(X)\n",
    "    #X=Conv2D(1,kernel_size=(2,3),strides=(2,2))(X)\n",
    "    #X=Activation(\"sigmoid\")(X)\n",
    "    X=Flatten()(X)\n",
    "    Y=Activation('softmax')(X)\n",
    "    model=Model(inputs=X_input,outputs=Y,name=\"JSmodel\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picmodel = Pimodel((10,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "picmodel.compile(optimizer=keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08,decay=0.0),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "picmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history=picmodel.fit(x=X_train,y=Y_train,batch_size=150,epochs=250,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = picmodel.evaluate(x=X_test,y=Y_test)\n",
    "print('Test Loss=' + str(preds[0]))\n",
    "print(\"Test Accuacy =\"+str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0, 1)) # Uncomment this when showing you model for pay raise\n",
    "plt.text(100,0.6,\"accuracy=\"+str(preds[1]))\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"accuary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.text(100,1.25,\"loss=\"+str(preds[0]))\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig(\"loss.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "picmodel.save(\"alphabet_model_20200729.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picmodel = load_model(\"./alphabet_model_20200729.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel_weights=np.squeeze(picmodel.get_weights())\n",
    "#print(kernel_weights.flatten())\n",
    "a1=np.squeeze(kernel_weights)[0:3,0:6].flatten()\n",
    "print(kernel_weights)\n",
    "N=abs(a1.flatten())[np.argmax(abs(a1.flatten()))]\n",
    "print(N)\n",
    "a1_nor=a1.flatten()/N\n",
    "print(a1_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "picmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the oupput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import numpy\n",
    "import random\n",
    "import os,sys\n",
    "def get_layer_output(model,x,index=0):\n",
    "    layer = K.function([model.input],[model.layers[index].output])\n",
    "    return layer([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import numpy\n",
    "import random\n",
    "import os,sys\n",
    "def get_layer_output(model,x,index=0):\n",
    "    layer = K.function([model.input],[model.layers[index].output])\n",
    "    return layer([x])[0]\n",
    "path=\"./dataset3/A/\"\n",
    "imlist=os.listdir(path)\n",
    "#rlist=random.sample(imlist,1000)\n",
    "one_t=[]\n",
    "One_t=[]\n",
    "fn_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/A/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    one_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"A\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        fn_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        One_t.append(elef) \n",
    "one1=collections.Counter(one_t)\n",
    "print(len(one_t),collections.Counter(one_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/B/\"\n",
    "imlist=os.listdir(path)\n",
    "#rlist=random.sample(imlist,1000)\n",
    "two_t=[]\n",
    "Two_t=[]\n",
    "B_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/B/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    two_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_4)]==\"B\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        B_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Two_t.append(elef)\n",
    "two1=collections.Counter(two_t)\n",
    "print(len(two_t),collections.Counter(two_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/C/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "three_t=[]\n",
    "Three_t=[]\n",
    "C_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/C/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    three_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_4)]==\"C\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        C_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Three_t.append(elef)\n",
    "three1=collections.Counter(three_t)\n",
    "print(len(three_t),collections.Counter(three_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][2]\n",
    "C_t.sort(key=takesecond,reverse=True)\n",
    "for i in C_t:\n",
    "    input_y=np.load(\"./dataset3/C/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\")\n",
    "    print(list(i[1]))\n",
    "    print(\"L2_with_Division\")\n",
    "    print(list(i[1]/N))\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Three_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset2/C/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/D/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "four_t=[]\n",
    "Four_t=[]\n",
    "D_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/D/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    four_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_2)]==\"D\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        D_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_4)])\n",
    "        Four_t.append(elef)\n",
    "four1=collections.Counter(four_t)\n",
    "print(len(four_t),collections.Counter(four_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][3]\n",
    "D_t.sort(key=takesecond,reverse=True)\n",
    "for i in D_t:\n",
    "    input_y=np.load(\"./dataset3/D/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\")\n",
    "    print(list(i[1]))\n",
    "    print(\"L2_with_Division\")\n",
    "    print(list(i[1]/N))\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Four_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset2/D/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/E/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "five_t=[]\n",
    "Five_t=[]\n",
    "E_t=[]\n",
    "#print(len(imlist))\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/E/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    five_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_4)]==\"E\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        E_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_4)])\n",
    "        Five_t.append(elef)\n",
    "five1=collections.Counter(five_t)\n",
    "print(len(five_t),collections.Counter(five_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][4]\n",
    "E_t.sort(key=takesecond,reverse=True)\n",
    "for i in E_t:\n",
    "    input_y=np.load(\"./dataset3/E/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\")\n",
    "    print(list(i[1]))\n",
    "    print(\"L2_with_Division\")\n",
    "    print(list(i[1]/N))\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Five_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset2/E/\"+elef)\n",
    "    np.save(elef,arr)\n",
    "    #print(arr,np.sum(arr==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/F/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "six_t=[]\n",
    "Six_t=[]\n",
    "F_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/F/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    six_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"F\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        F_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Six_t.append(elef)\n",
    "six1=collections.Counter(six_t)\n",
    "print(len(six_t),collections.Counter(six_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][5]\n",
    "F_t.sort(key=takesecond,reverse=True)\n",
    "for i in F_t:\n",
    "    input_y=np.load(\"./dataset3/F/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\")\n",
    "    print(list(i[1]))\n",
    "    print(\"L2_with_Division\")\n",
    "    print(list(i[1]/N))\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Six_t,514):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset2/F/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for ele in random.sample(Six_t,486):\n",
    "    input_x=np.load(r\"./dataset2/F/\"+ele)\n",
    "    input_y=input_x.flatten()\n",
    "    index1=np.argwhere(input_y==1)\n",
    "    index2=index1.flatten()\n",
    "    input_y[np.random.choice(index2,1)]=0\n",
    "    inputy1=input_y.reshape(10,10)\n",
    "    np.save(\"P\"+ele,input_x)\n",
    "    print(input_x)\n",
    "    print(inputy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/G/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "seven_t=[]\n",
    "Seven_t=[]\n",
    "G_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/G/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    seven_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"G\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        G_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Seven_t.append(elef)\n",
    "seven1=collections.Counter(seven_t)\n",
    "print(len(seven_t),collections.Counter(seven_t))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def takesecond(elem):\n",
    "    return elem[1][6]\n",
    "G_t.sort(key=takesecond,reverse=True)\n",
    "for i in G_t:\n",
    "    input_y=np.load(\"./dataset3/G/\"+i[0])\n",
    "    print(i[0])\n",
    "    print(\"L2\")\n",
    "    print(list(i[1]))\n",
    "    print(\"L2_with_Division\")\n",
    "    print(list(i[1]/N))\n",
    "    print(input_y,np.sum(input_y==1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for elef in random.sample(Seven_t,1000):\n",
    "    #print(elef)\n",
    "    arr=np.load(\"./dataset2/G/\"+elef)\n",
    "    np.save(elef,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./dataset3/H/\"\n",
    "imlist=os.listdir(path)\n",
    "#rmlist=random.sample(imlist,1000)\n",
    "eight_t=[]\n",
    "Eight_t=[]\n",
    "H_t=[]\n",
    "for elef in imlist:\n",
    "    input_y=np.load(\"./dataset3/H/\"+elef)\n",
    "    input_x = np.expand_dims(input_y,axis=0)\n",
    "    input_x = np.expand_dims(input_x,axis=3)\n",
    "    layer_1 = get_layer_output(picmodel,input_x,index=1)\n",
    "    layer_2 = get_layer_output(picmodel,input_x,index=2)\n",
    "    layer_3 = get_layer_output(picmodel,input_x,index=3)\n",
    "    layer_4 = get_layer_output(picmodel,input_x,index=4)\n",
    "    layer_5 = get_layer_output(picmodel,input_x,index=5)\n",
    "    eight_t.append(class_names[np.argmax(layer_5)])\n",
    "    if class_names[np.argmax(layer_5)]==\"H\":\n",
    "        FN_t=[]\n",
    "        FN_t.append(elef)\n",
    "        FN_t.append(layer_2.flatten())\n",
    "        H_t.append(FN_t)\n",
    "        print(elef,layer_2.flatten(),class_names[np.argmax(layer_5)])\n",
    "        Eight_t.append(elef)\n",
    "eight1=collections.Counter(eight_t)\n",
    "print(len(eight_t),collections.Counter(eight_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "current_palette = sns.color_palette()\n",
    "#dict1 = pd.DataFrame(pd.Series(f1),columns=[\"numbers\"],index=[\"fire\",\"soil\",\"water\",\"wood\",\"gas\",\"sky\",\"human\",\"life\"])\n",
    "x=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "dict1=pd.Series(one1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict1=dict1.fillna(0)\n",
    "dict2=pd.Series(two1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict2=dict2.fillna(0)\n",
    "dict3=pd.Series(three1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict3=dict3.fillna(0)\n",
    "dict4=pd.Series(four1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict4=dict4.fillna(0)\n",
    "#dict4[np.squeeze(np.argwhere(np.isnan(dict4)))]=0\n",
    "dict5=pd.Series(five1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict5=dict5.fillna(0)\n",
    "#dict5[np.squeeze(np.argwhere(np.isnan(dict5)))]=0\n",
    "dict6=pd.Series(six1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict6=dict6.fillna(0)\n",
    "#dict6[np.squeeze(np.argwhere(np.isnan(dict6)))]=0\n",
    "dict7=pd.Series(seven1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict7=dict7.fillna(0)\n",
    "#dict7[np.squeeze(np.argwhere(np.isnan(dict7)))]=0\n",
    "dict8=pd.Series(eight1).reindex([\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"])\n",
    "dict8=dict8.fillna(0)\n",
    "#dict8[np.squeeze(np.argwhere(np.isnan(dict8)))]=0\n",
    "plt.figure(figsize=(24,10))\n",
    "plt.subplot(241)\n",
    "#plt.bar(x,dict1,color=\"red\")\n",
    "sns.barplot(x,dict1,palette=\"deep\")\n",
    "plt.title(\"A regonition\")\n",
    "for i,j in zip(range(8), dict1):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(242)\n",
    "sns.barplot(x,dict2,palette=\"deep\")\n",
    "plt.title(\"B regonition\")\n",
    "for i,j in zip(range(8), dict2):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict2)\n",
    "plt.subplot(243)\n",
    "sns.barplot(x,dict3,palette=\"deep\")\n",
    "plt.title(\"C regonition\")\n",
    "for i,j in zip(range(8), dict3):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(244)\n",
    "sns.barplot(x,dict4,palette=\"deep\")\n",
    "plt.title(\"D regonition\")\n",
    "for i,j in zip(range(8), dict4):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(245)\n",
    "sns.barplot(x,dict5,palette=\"deep\")\n",
    "plt.title(\"E regonition\")\n",
    "for i,j in zip(range(8), dict5):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(246)\n",
    "sns.barplot(x,dict6,palette=\"deep\")\n",
    "plt.title(\"F regonition\")\n",
    "for i,j in zip(range(8), dict6):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(247)\n",
    "sns.barplot(x,dict7,palette=\"deep\")\n",
    "plt.title(\"G regonition\")\n",
    "for i,j in zip(range(8), dict7):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "plt.subplot(248)\n",
    "sns.barplot(x,dict8,palette=\"deep\")\n",
    "plt.title(\"H regonition\")\n",
    "for i,j in zip(range(8), dict8):\n",
    "        plt.text(i, j, str(int(j)), ha='center', va='bottom', fontsize=10.5)\n",
    "#print(dict1)\n",
    "#list1=dict1.values.tolist()\n",
    "plt.savefig(\"regonition.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
